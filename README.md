# PClub_secy_task

Approach applied: In this NLP problem, I used DPR tokenizers and encoders for both questions and contexts, and the BART tokenizer and generator for generating answers. All paragraphs are encoded into dense vector embeddings using the DPR context encoder. For each query, the model encodes the query using the DPR question encoder and computes cosine similarity scores between the query vector and paragraph embeddings to retrieve the top 5 most relevant paragraphs. Using the BART model, it generates a response by combining the query with each retrieved paragraph and producing an answer. The model processes each query to retrieve relevant paragraphs and attempts to generate a non-empty answer from the most relevant paragraph. The model, thus, then predicts the answer to query asked and for simplicity i ahve implemented gradio interface as well. The approach I used mostly follows from the pdfs that were sent along with teh task. I tried the RAG tokenizer, generator and encoder first but it worked really slow on my laptop because it was using cpu instead of gpu. So i went with a comparitively loght transformer, sentence transformer and then used DPR and BART tokenizer as said above.
